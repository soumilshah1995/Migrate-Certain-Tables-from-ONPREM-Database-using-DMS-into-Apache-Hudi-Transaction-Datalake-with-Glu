{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%connections hudi-connection\n%glue_version 3.0\n%region us-west-2\n%worker_type G.1X\n%number_of_workers 3\n%spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4fb72b7d-c4af-4ecc-9154-5b1734b79c27.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Connections to be included:\nhudi-connection\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4fb72b7d-c4af-4ecc-9154-5b1734b79c27.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4fb72b7d-c4af-4ecc-9154-5b1734b79c27.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous region: us-west-2\nSetting new region to: us-west-2\nReauthenticating Glue client with new region: us-west-2\nIAM role has been set to arn:aws:iam::043916019468:role/Lab3. Reauthenticating.\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::043916019468:role/Lab3\nAuthentication done.\nRegion is set to: us-west-2\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4fb72b7d-c4af-4ecc-9154-5b1734b79c27.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4fb72b7d-c4af-4ecc-9154-5b1734b79c27.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 3\nSetting new number of workers to: 3\nPrevious Spark configuration: spark.serializer=org.apache.spark.serializer.KryoSerializer\nSetting new Spark configuration to: spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 4fb72b7d-c4af-4ecc-9154-5b1734b79c27\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "try:\n    import sys\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    from pyspark.sql.session import SparkSession\n    from awsglue.dynamicframe import DynamicFrame\n    from pyspark.sql.functions import col, to_timestamp, monotonically_increasing_id, to_date, when\n    from pyspark.sql.functions import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.sql.types import *\n    from datetime import datetime\n    import boto3\n    from functools import reduce\nexcept Exception as e:\n    pass",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::043916019468:role/Lab3\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 3\nSession ID: 4b821d65-f6c0-427c-9776-023a70fbc5c8\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer\nWaiting for session 4b821d65-f6c0-427c-9776-023a70fbc5c8 to get into ready status...\nSession 4b821d65-f6c0-427c-9776-023a70fbc5c8 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 1:  Create Spark Session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark = SparkSession.builder.config('spark.serializer','org.apache.spark.serializer.KryoSerializer').config('spark.sql.hive.convertMetastoreParquet','false').config('spark.sql.legacy.pathOptionBehavior.enabled', 'true').getOrCreate()\nsc = spark.sparkContext\nglueContext = GlueContext(sc)\njob = Job(glueContext)\nlogger = glueContext.get_logger()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ReadDF = glueContext.create_dynamic_frame.from_catalog(\n    database=\"dmsdb\",\n    table_name=\"raw_sales\",\n    transformation_ctx=\"ReadDF\",\n)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ChangeSchemaApplyMapping = ApplyMapping.apply(\n    frame=ReadDF,\n    mappings=[\n        (\"action\", \"string\", \"action\", \"string\"),\n        (\"invoiceid\", \"long\", \"invoiceid\", \"long\"),\n        (\"itemid\", \"long\", \"itemid\", \"long\"),\n        (\"category\", \"string\", \"category\", \"string\"),\n        (\"price\", \"long\", \"price\", \"long\"),\n        (\"quantity\", \"long\", \"quantity\", \"long\"),\n        (\"orderdate\", \"string\", \"orderdate\", \"string\"),\n        (\"destinationstate\", \"string\", \"destinationstate\", \"string\"),\n        (\"shippingtype\", \"string\", \"shippingtype\", \"string\"),\n        (\"referral\", \"string\", \"referral\", \"string\"),\n    ],\n    transformation_ctx=\"ChangeSchemaApplyMapping\",\n)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df = ChangeSchemaApplyMapping.toDF()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+---------+------+---------+-----+--------+--------------------+----------------+------------+----------------+\n|action|invoiceid|itemid| category|price|quantity|           orderdate|destinationstate|shippingtype|        referral|\n+------+---------+------+---------+-----+--------+--------------------+----------------+------------+----------------+\n|     I|     2469|     9|   Garden|   58|       3|2016-07-17 00:00:...|              IL|        Free|Friend/Colleague|\n|     I|    10370|    68|   Office|   31|       4|2016-01-21 00:00:...|              NJ|        Free|           Other|\n|     I|    15061|    54|   Garden|   57|       2|2016-10-17 00:00:...|              RI|       2-Day|       Online Ad|\n|     I|    15425|    55|   Office|   61|       3|2016-07-18 00:00:...|              HI|       3-Day|       Online Ad|\n|     I|    18229|    15|  Kitchen|   20|       2|2016-09-26 00:00:...|              PA|        Free|           Other|\n|     I|    18075|    67|   Garden|    4|       2|2016-12-15 00:00:...|              MD|       2-Day|           Other|\n|     I|    12292|    85|   Office|    2|       1|2016-07-18 00:00:...|              CO|        Free|           Other|\n|     I|    11569|    24|   Garden|    2|       4|2016-03-15 00:00:...|              UT|        Free| Repeat Customer|\n|     I|      109|    52|   Garden|   16|       3|2016-10-21 00:00:...|              RI|       2-Day|       Online Ad|\n|     I|     7779|    16|   Office|   48|       4|2016-07-22 00:00:...|              AK|       2-Day| Repeat Customer|\n|     U|     9218|    94|Household|   21|       3|2016-03-20 00:00:...|              NC|      2 days|Friend/Colleague|\n|     U|       14|    92|   Office|   88|       1|2016-03-27 00:00:...|              HI|      2 days| Repeat Customer|\n|     U|     9218|    94|Household|   21|       3|2016-03-20 00:00:...|              NC|     21 days|Friend/Colleague|\n|     U|       14|    92|   Office|   88|       1|2016-03-27 00:00:...|              HI|     21 days| Repeat Customer|\n|     U|     9218|    94|Household|   21|       3|2016-03-20 00:00:...|              NC|     21 days|Friend/Colleague|\n|     U|       14|    92|   Office|   88|       1|2016-03-27 00:00:...|              HI|     21 days| Repeat Customer|\n|     I|    15248|    11|Household|   15|       4|2016-10-25 00:00:...|              AL|       2-Day|       Online Ad|\n|     U|     9218|    94|Household|   21|       3|2016-03-20 00:00:...|              NC|    231 days|Friend/Colleague|\n|     U|       14|    92|   Office|   88|       1|2016-03-27 00:00:...|              HI|    231 days| Repeat Customer|\n|     U|     9218|    94|Household|   21|       3|2016-03-20 00:00:...|              NC|    231 days|Friend/Colleague|\n+------+---------+------+---------+-----+--------+--------------------+----------------+------------+----------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Write into HUDI ",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "db_name = \"dmsdb\"\ntable_name=\"sales\"\n\nrecordkey = 'invoiceid'\nprecombine = 'action'\n\npath = \"s3://soumil-dms-learn/hudi/sales/\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ApacheHudiConnector0101forAWSGlue30_node1671045598524 = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options={\n            \"path\": path,\n            \"connectionName\": \"hudi-connection\",\n\n            \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n            'className': 'org.apache.hudi',\n            'hoodie.table.name': table_name,\n            'hoodie.datasource.write.recordkey.field': recordkey,\n            'hoodie.datasource.write.table.name': table_name,\n            'hoodie.datasource.write.operation': 'upsert',\n            'hoodie.datasource.write.precombine.field': precombine,\n\n\n            'hoodie.datasource.hive_sync.enable': 'true',\n            \"hoodie.datasource.hive_sync.mode\":\"hms\",\n            'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n            'hoodie.datasource.hive_sync.database': db_name,\n            'hoodie.datasource.hive_sync.table': table_name,\n            'hoodie.datasource.hive_sync.use_jdbc': 'false',\n            'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n            'hoodie.datasource.write.hive_style_partitioning': 'true',\n        },\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o134.pyWriteDynamicFrame.\n: org.apache.hudi.exception.HoodieException: Config conflict(key\tcurrent value\texisting value):\nPreCombineKey:\taction\titemid\n\tat org.apache.hudi.HoodieWriterUtils$.validateTableConfig(HoodieWriterUtils.scala:162)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:89)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat com.amazonaws.services.glue.marketplace.connector.SparkCustomDataSink.writeDynamicFrame(CustomDataSink.scala:45)\n\tat com.amazonaws.services.glue.DataSink.pyWriteDynamicFrame(DataSink.scala:71)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}